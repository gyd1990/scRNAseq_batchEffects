---
title: "Batch Effect Exploration"
subtitle: "Modeling FDR Control of edgeR"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 10
    fig_height: 7
    theme: cosmo
---




# Introduction

In this script models are built to predict FDR control from a dataset.
More specifically FDR control loss is predicted of edgeR using summed up cells.

## Load

First, libraries are loaded.

```{r, message=FALSE, results='hide'}
library(ggplot2) # plotting
library(data.table) # database-like tables
library(mgcv) # GAM
library(plotmo) # partial dependency plot
library(xgboost) # gradient boosting
library(scatterplot3d) # 3d plots
```

Objects were prepared in the pre-processing step and are loaded.

```{r}
AUC <- readRDS("AUC.rds")
FDR <- readRDS("FDR.rds")
noCall <- readRDS("noCall.rds")
```

## Split Data

1/3 split into train and test.
The training set will be used for model fitting and evaluation
while the test set will be used for prediction error estimation.

```{r}
FDR <- FDR[method == "edgeR_sum" & type == "full", ]
set.seed(42)
split <- sample(1:3, nrow(FDR), replace = TRUE)
Xtrain <- FDR[split %in% 1:2, !"D_FDR"]
Xtest <- FDR[split == 3, !"D_FDR"]
Ytrain <- FDR[split %in% 1:2, D_FDR]
Ytest <- FDR[split == 3, D_FDR]
```

In general from the pre processing it seems that predicting the FDR control
loss for edgeR should be easier than for scDD.
The final scDD model did not use $kBET_{rate}$ so I will try to avoid it
here as well.
Variable $\hat n$ should also not play a role when summing up cells.

```{r}
feats <- c("cv_hat_within", "cv_hat_between", "batch_corrs", "predFDR")
```














***















# Training

Generally, from the pre processing plots I assume predicting edgeR's
loss of FDR control is much easier.
So, here I will focus on being consistent with the model family used in
the scDD predicor.

## GAM

Again the `mgcv` package from Simon Wood is used.
I will use the same strategy as for fitting scDD.

```{r}
dt <- cbind(Xtrain, Ytrain)
names(dt)[ncol(dt)] <- "D_FDR"
fit <- gam(
  D_FDR ~ 
    s(cv_hat_within, k = 10) +
    s(cv_hat_between, k = 10) +
    s(predFDR, k = 3) +
    ti(cv_hat_within, cv_hat_between, k = 7) +
    ti(cv_hat_within, batch_corrs, k = 7) +
    ti(cv_hat_within, predFDR, k = 3) +
    ti(cv_hat_between, batch_corrs, k = 7) +
    ti(cv_hat_between, predFDR, k = 3) +
    ti(batch_corrs, predFDR, k = 3),
  data = dt,
  method = "ML"
)
summary(fit)
```

This can be extended to to..

```{r}
fit2 <- gam(
  D_FDR ~ 
    s(cv_hat_within, k = 10) +
    ti(cv_hat_within, cv_hat_between, k = 7) +
    ti(cv_hat_within, batch_corrs, k = 7) +
    ti(cv_hat_within, predFDR, k = 3) +
    te(batch_corrs, predFDR, cv_hat_between, k = 3),
  data = dt,
  method = "ML"
)
summary(fit2)
```

or..

```{r}
fit3 <- gam(
  D_FDR ~ 
    s(predFDR, k = 3) +
    ti(cv_hat_between, predFDR, k = 3) +
    ti(cv_hat_within, predFDR, k = 3) +
    ti(batch_corrs, predFDR, k = 3) +
    te(cv_hat_within, cv_hat_between, batch_corrs, k = 8),
  data = dt,
  method = "ML"
)
summary(fit3)
```

**Final Model**

```{r}
AIC(fit, fit2, fit3)
```

Model 3 it is.
The statistic below tells us there is some structure left in the data.
However, the plots look ok.

```{r}
gam.mod <- fit3
par(mfrow = c(2, 2))
gam.check(gam.mod)
```

A partial dependency plot of the final gam is shown below.

```{r, message=FALSE, warning=FALSE}
plotmo(gam.mod, pmethod = "partdep", nrug = "density",
       pt.col = 1, smooth.col = 2, ndiscrete = 0)
```















## xgboost

Now, this might be a bit overkill but as with scDD I will train a
tree ensemble by stochastic gradient boosting.
Again the `xgboost` implementation is used.

```{r}
dtrain <- xgb.DMatrix(
  data = as.matrix(Xtrain[, feats, with = FALSE]), 
  label = Ytrain
)
```

**Parameter Tuning**

For hyper parameter estimation I will create a function that iteratively
trains a model with different hyper parameter combinations.
The performance is monitored using cross validation.
Theoretically, up to 100k trees can be built per model, but I added an early
stopping condition which will terminate the process if for 1000 iterations
the RMSE did not decrease.
The model with the lowest cross validated mean RMSE will be noted for each
parameter combination.

```{r}
train.grid <- function(train, pars, n = 100000, objFun = NULL, nstop = 1000) {
  N <- nrow(pars)
  rmse <- data.frame(iter = integer(N), mean = numeric(N), sd = numeric(N))
  for (i in seq_len(N)) {
    if (i %% 50 == 1) print(sprintf("Starting %d of %d", i, N))
    sink("/dev/null")
    fit <- xgb.cv(params = lapply(pars, function(x) x[i]), data = train, 
                  nrounds = n, nfold = 5, obj = objFun, 
                  early_stopping_rounds = nstop, nthread = 4)
    sink()
    dt <- fit$evaluation_log
    idx <- which.min(dt$test_rmse_mean)
    rmse$iter[i] <- idx
    rmse$mean[i] <- dt$test_rmse_mean[idx]  
    rmse$sd[i] <- dt$test_rmse_std[idx] 
  }
  return(rmse)
}
```

Initially, only the most influencial parameters will be tested in a
random search.
These are the parameters which mainly define how complex each tree can grow:
minimum RMSE icnrease per partition, maximum number of partitions,
minimum size per partition.
Other hyper parameters are set to reasonable default values.

```{r}
set.seed(42)
pars.grid1 <- expand.grid(
  eta = 0.01, # learning rate
  gamma = runif(5, max = .1), # minimum RMSE increase for partiion
  max_depth = sample(1:12, 5), # maximum partiions
  min_child_weight = sample(1:(nrow(dtrain) / 10), 5), # minimum partition
  alpha = 0, # L1 regularization
  subsample = .5, # row subsampling
  colsample_bytree = 1 # column subsampling
)
```

```{r grid1.1}
res <- train.grid(dtrain, pars.grid1)
```

Below is the parameter combination with the best cross validated mean RMSE
and a 3d plot of the joint distribution of hyper parameters and mean RMSE.

```{r}
rmse1 <- setDT(cbind(res, pars.grid1))
(best1 <- rmse1[which.min(mean), ])
scatterplot3d(rmse1$min_child_weight, rmse1$gamma, rmse1$mean, 
              color = rmse1$max_depth, angle = 60, pch = 20, type = "h")
md <- sort(unique(rmse1$max_depth))
legend("topleft", title = "max_depth", bty = "n", legend = md, fill = md)
```

Now regularization hyper parameters are tested: 
Lasso regularization for weights, row subsampling and 
column (feature) subsampling for each tree.

```{r}
set.seed(42)
pars.grid2 <- expand.grid(
  eta = 0.01, # learning rate
  gamma = best1$gamma, # minimum RMSE increase for partiion
  max_depth = best1$max_depth, # maximum partiions
  min_child_weight = best1$min_child_weight, # minimum partition
  alpha = runif(5, max = .5), # L1 regularization
  subsample = runif(5, min = .1, max = .5), # row subsampling
  colsample_bytree = .5 # column subsampling
)
```

```{r grid1.2}
res <- train.grid(dtrain, pars.grid2)
```

```{r}
rmse2 <- setDT(cbind(res, pars.grid2))
(best2 <- rmse2[which.min(mean), ])
```

The final best combination of hyper parameters is...

```{r}
rmse <- rbind(rmse1, rmse2)
(best <- rmse[which.min(mean), ])
```

The final model is fitted below.

```{r}
parsl <- list(
  eta = 0.01, # learning rate
  gamma = best$gamma, # minimum RMSE increase for partiion
  max_depth = best$max_depth, # maximum partiions
  min_child_weight = best$min_child_weight, # minimum partition
  alpha = best$alpha, # L1 regularization
  subsample = best$subsample, # row subsampling
  colsample_bytree = best$colsample_bytree # column subsampling
)
fit <- xgb.train(params = parsl, data = dtrain, nrounds = best$iter)
```

Below you can see a ranking of feature importance.
It is based on the number of occurences and their individual impacts of
a feature in partitions of the tree ensemble.

```{r}
dt <- xgb.importance(colnames(dtrain), fit)
xgb.plot.importance(dt)
xgb.mod <- fit
```













***














# Evaluation

Evaluation of the 2 models is done on the test set, 
which was not used during training or validation.

```{r}
gam.pred <- predict(gam.mod, Xtest)
dtest <- xgb.DMatrix(data = as.matrix(Xtest[, feats, with = FALSE]))
xgb.pred <- predict(xgb.mod, dtest)
```

The residuals of all models are shown below 
together with cubic regression splines.
Well, there is clearly some structure not captured by the models.

```{r, warning=FALSE}
dt <- data.table(
  D_FDR = rep(Ytest, 2),
  residual = c(Ytest - gam.pred, Ytest - xgb.pred),
  model = rep(c("GAM", "XGBoost"), each = length(Ytest))
)
ggplot(dt, aes(x = D_FDR, y = residual)) +
  geom_point(aes(color = model), alpha = .3, size = 1) +
  geom_smooth(aes(color = model, fill = model)) +
  theme_bw()
```

Standard errors are calculated and shown below.

```{r}
err <- dt[, .(se = sd(residual) / sqrt(length(residual))), by = model]
ggplot(err, aes(x = model, y = se)) +
  geom_pointrange(aes(ymin = 0, ymax = se), color = "blue") +
  coord_flip() +
  theme_bw()
(err)
```

Confidence intervalls are shown below.

```{r}
err[, .(CI.99 = 2.58 * se, CI.95 = 1.96 * se), by = model]
```














***













# Save

Now the final models are trained on the whole dataset and saved.

**GAM**

```{r}
gam.mod <- gam(
  D_FDR ~ te(cv_hat_within, cv_hat_between, batch_corrs, k = 7),
  data = FDR,
  method = "ML"
)
saveRDS(gam.mod, "edgeR_gam.mod.rds")
```

**XGBoost**

```{r}
dtrain <- xgb.DMatrix(
  data = as.matrix(FDR[, feats, with = FALSE]), 
  label = FDR$D_FDR
)
xgb.mod <- xgb.train(params = parsl, data = dtrain, nrounds = best$iter)
saveRDS(xgb.mod, "edgeR_xgb.mod.rds")
```





















***












# Session Info

```{r}
sessionInfo()
```


