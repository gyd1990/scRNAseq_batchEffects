---
title: "Batch Effect Exploration"
subtitle: "Modeling FDR Control of scDD"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 10
    fig_height: 7
    theme: cosmo
---




# Introduction

In this script I try to predict the loss of FDR for scDD using pooled cells
with Kolmogorov-Smirnov test.

## Load

First, libraries are loaded.

```{r, message=FALSE, results='hide'}
library(ggplot2) # plotting
library(data.table) # database-like tables
library(earth) # MARS
library(mgcv) # GAM
library(xgboost) # gradient boosting
library(scatterplot3d) # 3d plots
```

Objects were prepared in the pre-processing step and are loaded.

```{r}
AUC <- readRDS("AUC.rds")
FDR <- readRDS("FDR.rds")
noCall <- readRDS("noCall.rds")
```

## Split Data

1/3 split into train and test.
The training set will be used for model fitting and evaluation
while the test set will be used for prediction error estimation.

```{r}
FDR <- FDR[method == "scDD_pool_ks" & type == "full", ]
set.seed(42)
split <- sample(1:3, nrow(FDR), replace = TRUE)
Xtrain <- FDR[split %in% 1:2, !"D_FDR"]
Xtest <- FDR[split == 3, !"D_FDR"]
Ytrain <- FDR[split %in% 1:2, D_FDR]
Ytest <- FDR[split == 3, D_FDR]
```

Predictor variables.

```{r}
feats <- c("n_hat", "cv_hat_within", "cv_hat_between", 
           "kBET_rate", "batch_corrs", "predFDR")
```














***















# Training

## MARS

A Multivariate  Adaptive Regression Splines (MARS) model is fit to the data.
As there are not too many variables and the dataset is not so big,
an exhaustive term selection is done during the pruning method and repeated
cross validation is used to estimate R squared.
The method is forced to stop during the forward pass if the estimated R squared
does not increase by 0.001. 
Interaction terms are allowed if the respective first degree terms are already
in the model.

```{r, results='hide'}
fit <- earth(Xtrain[, feats, with = FALSE], Ytrain,
             degree = 2, nk = 100, thresh = 0.001,
             pmethod = "exhaustive", nfold = 5, ncross = 10)
```

```{r}
summary(fit)
```

**Reduced**

Next I try to reduce the model by removing features.
I do this in a backwards selection fashion.
Below is the model that yielded a cross validated R squared
within 1 standard deviation of the full model.

```{r, results='hide'}
fit2 <- earth(Xtrain[, feats[-4], with = FALSE], Ytrain,
             degree = 2, nk = 100, thresh = 0.001,
             pmethod = "exhaustive", nfold = 5, ncross = 10)
```

```{r}
summary(fit2)
```

The reduced model actually has a higher cross validated R squared.
$kBET_{rate}$ was removed from the set of features.

The model included some interaction terms.
Though $\hat{cv}_{between} *batch_{corrs}$ seems questionable with coefficients
over 100; These terms are highly correlated.

In a third model I want to exclude these questionable interactions with the 
function below.

```{r}
allowed <- function(degree, pred, parents, namesx) {
  if (degree < 2) return(TRUE)
  idx1 <- which(namesx == "cv_hat_between")
  idx2 <- which(namesx == "batch_corrs")
  if ((pred == idx1 && parents[idx2]) || (pred == idx2 && parents[idx1])) {
    return(FALSE)
  }
  return(TRUE)
}
```

```{r, results='hide'}
fit3 <- earth(Xtrain[, feats[-4], with = FALSE], Ytrain, 
             degree = 2, nk = 100, thresh = 0.001, allowed = allowed,
             pmethod = "exhaustive", nfold = 5, ncross = 10)
```

```{r}
summary(fit3)
```

The cross validated error is still within 1 standard deviation of the best
model and the rediculously high coefficients are gone.

**Final Model**

The last fit is used as final model.

```{r}
mars.mod <- fit3
plot(mars.mod)
```

Most residuals are below 0.1, which would be 10% error in the prediction 
of FDR control loss.
Unfortunately, most values are around 0 and 1, and the values in between
are predicted quite quite bad.

Partial dependency plots of the mdoel are shown below.

```{r, message=FALSE, warning=FALSE}
plotmo(mars.mod, pmethod = "partdep", nrug = "density",
       pt.col = 1, smooth.col = 2, ndiscrete = 0)
```












## GAM

Next, I try to fit a generalized additive model (gam) to the data.
I will use the `mgcv` package from Simon Wood for this.

My strategy here is a backwards selection starting form a full model with
many effective degrees of freedom (`k`) and maximum degree 2 interactions.
For single variables thin-plate regression splines, for more variables
natural cubic regression splines of tensor products is used as basis expansion.
Parameter estimation is done via *Maximum Likelihood* as recommended in the
documentation (for backwards selection).
In the model below I already deleted all terms which where not significant 
(below 5\%) and `k` was reduced without decreasing adjusted R squared.

```{r}
dt <- cbind(Xtrain, Ytrain)
names(dt)[ncol(dt)] <- "D_FDR"
fit <- gam(
  D_FDR ~ 
    s(n_hat, k = 10) +
    s(cv_hat_within, k = 10) +
    s(batch_corrs, k = 10) +
    s(predFDR, k = 3) +
    ti(predFDR, n_hat, k = 3) +
    ti(predFDR, cv_hat_between, k = 3) +
    ti(cv_hat_between, n_hat, k = 7) +
    ti(batch_corrs, n_hat, k = 7) +
    ti(batch_corrs, cv_hat_between, k = 7) +
    ti(cv_hat_within, cv_hat_between, k = 7),
  data = dt,
  method = "ML"
)
summary(fit)
```

The plots and the statistic below shows that there is still a lot of structure
left which is captured by the model.

```{r}
par(mfrow = c(2, 2))
gam.check(fit)
```

**Reduced**

I further reduced the degrees of freedom while keeping R squared above 0.933.

```{r}
fit2 <- gam(
  D_FDR ~ 
    s(n_hat, k = 5) +
    s(cv_hat_within, k = 5) +
    s(batch_corrs, k = 5) +
    s(predFDR, k = 3) +
    ti(predFDR, n_hat, k = 3) +
    ti(predFDR, cv_hat_between, k = 3) +
    ti(cv_hat_between, n_hat, k = 6) +
    ti(batch_corrs, n_hat, k = 5) +
    ti(batch_corrs, cv_hat_between, k = 7) +
    ti(cv_hat_within, cv_hat_between, k = 5),
  data = dt,
  method = "ML"
)
summary(fit2)
```

If I include a 3-way interaction term as full tensor product (`te`)
I can remove many terms.

```{r}
fit3 <- gam(
  D_FDR ~ 
    s(cv_hat_within, k = 5) +
    s(batch_corrs, k = 5) +
    ti(batch_corrs, n_hat, k = 5) +
    ti(batch_corrs, cv_hat_between, k = 7) +
    ti(cv_hat_within, cv_hat_between, k = 5) +
    te(n_hat, cv_hat_between, predFDR, k = 3),
  data = dt,
  method = "ML"
)
summary(fit3)
```

The statistics shown below still indicates that there is some unexplained
structure left.
However, increasing `k` merely helped, while fitting time exploded.

```{r}
par(mfrow = c(2, 2))
gam.check(fit3)
```

**Final Model**

As suggested in the `mgcv` vignette I will use the AIC for model selection.

```{r}
AIC(fit, fit2, fit3)
```

Thereafter, the full model will be used.
A partial dependency plot is shown below.

```{r, message=FALSE, warning=FALSE}
gam.mod <- fit
plotmo(gam.mod, pmethod = "partdep", nrug = "density",
       pt.col = 1, smooth.col = 2, ndiscrete = 0)
```















## xgboost

Finally, I want to try out a quite powerful method which creates an ensemble
of trees.
Here, regression trees are iteratively fitted to minimize the RMSE 
using stochastic gradient boosting.
Trees are quite flexible with any kind of data and their ensemble can be
very accurate.
Overfitting is avoided by subsampling and explicit regularization.
I will use the `xgboost` implementation.

As $kBET_{rate}$ was removed by the other models I will also leave it out here.

```{r}
dtrain <- xgb.DMatrix(
  data = as.matrix(Xtrain[, feats[-4], with = FALSE]), 
  label = Ytrain
)
```

**Parameter Tuning**

For hyper parameter estimation I will create a function that iteratively
trains a model with different hyper parameter combinations.
The performance is monitored using cross validation.
Theoretically, up to 100k trees can be built per model, but I added an early
stopping condition which will terminate the process if for 1000 iterations
the RMSE did not decrease.
The model with the lowest cross validated mean RMSE will be noted for each
parameter combination.

```{r}
train.grid <- function(train, pars, n = 100000, objFun = NULL, nstop = 1000) {
  N <- nrow(pars)
  rmse <- data.frame(iter = integer(N), mean = numeric(N), sd = numeric(N))
  for (i in seq_len(N)) {
    if (i %% 50 == 1) print(sprintf("Starting %d of %d", i, N))
    sink("/dev/null")
    fit <- xgb.cv(params = lapply(pars, function(x) x[i]), data = train, 
                  nrounds = n, nfold = 5, obj = objFun, 
                  early_stopping_rounds = nstop, nthread = 4)
    sink()
    dt <- fit$evaluation_log
    idx <- which.min(dt$test_rmse_mean)
    rmse$iter[i] <- idx
    rmse$mean[i] <- dt$test_rmse_mean[idx]  
    rmse$sd[i] <- dt$test_rmse_std[idx] 
  }
  return(rmse)
}
```

Initially, only the most influencial parameters will be tested in a
random search.
These are the parameters which mainly define how complex each tree can grow:
minimum RMSE icnrease per partition, maximum number of partitions,
minimum size per partition.
Other hyper parameters are set to reasonable default values.

```{r}
set.seed(42)
pars.grid1 <- expand.grid(
  eta = 0.01, # learning rate
  gamma = runif(5, max = .1), # minimum RMSE increase for partiion
  max_depth = sample(1:12, 5), # maximum partiions
  min_child_weight = sample(1:(nrow(dtrain) / 10), 5), # minimum partition
  alpha = 0, # L1 regularization
  subsample = .5, # row subsampling
  colsample_bytree = .5 # column subsampling
)
```

```{r grid1.1}
res <- train.grid(dtrain, pars.grid1)
```

Below is the parameter combination with the best cross validated mean RMSE
and a 3d plot of the joint distribution of hyper parameters and mean RMSE.

```{r}
rmse1 <- setDT(cbind(res, pars.grid1))
(best1 <- rmse1[which.min(mean), ])
scatterplot3d(rmse1$min_child_weight, rmse1$gamma, rmse1$mean, 
              color = rmse1$max_depth, angle = 60, pch = 20, type = "h")
md <- sort(unique(rmse1$max_depth))
legend("topleft", title = "max_depth", bty = "n", legend = md, fill = md)
```

Now regularization hyper parameters are tested: 
Lasso regularization for weights, row subsampling and 
column (feature) subsampling for each tree.

```{r}
set.seed(42)
pars.grid2 <- expand.grid(
  eta = 0.01, # learning rate
  gamma = best1$gamma, # minimum RMSE increase for partiion
  max_depth = best1$max_depth, # maximum partiions
  min_child_weight = best1$min_child_weight, # minimum partition
  alpha = runif(5, max = .5), # L1 regularization
  subsample = runif(5, min = .1, max = .5), # row subsampling
  colsample_bytree = runif(5, min = .1, max = .5) # column subsampling
)
```

```{r grid1.2}
res <- train.grid(dtrain, pars.grid2)
```

```{r}
rmse2 <- setDT(cbind(res, pars.grid2))
(best2 <- rmse2[which.min(mean), ])
```

The final best combination of hyper parameters is...

```{r}
rmse <- rbind(rmse1, rmse2)
(best <- rmse[which.min(mean), ])
```

Eventually, one can increase the performance by making the model bigger,
i.e. reducing the learning rate and increasing the number of iterations.
I have tried that by decreasing $\eta$ to 0.001 or 0.05 while increasing the
maximum number of iterations to 100k and relaxing the early stopping to
10k.
However, validation RMSE improvements only start near 100k trees per model.
I decided to go with the much lighter model with nearly the same performance.

```{r, results='hide'}
parsl <- list(
  eta = 0.01, # learning rate
  gamma = best$gamma, # minimum RMSE increase for partiion
  max_depth = best$max_depth, # maximum partiions
  min_child_weight = best$min_child_weight, # minimum partition
  alpha = best$alpha, # L1 regularization
  subsample = best$subsample, # row subsampling
  colsample_bytree = best$colsample_bytree # column subsampling
)
```

The final model is fitted below.

```{r}
xgb.mod <- xgb.train(params = parsl, data = dtrain, nrounds = best$iter)
```

Below you can see a ranking of feature importance.
It is based on the number of occurences and their individual impacts of
a feature in partitions of the tree ensemble.

```{r}
dt <- xgb.importance(colnames(dtrain), xgb.mod)
xgb.plot.importance(dt)
```













***














# Evaluation

Evaluation of the 3 models is done on the test set, 
which was not used during training or validation.

```{r}
mars.pred <- predict(mars.mod, Xtest[, feats[-4], with = FALSE])
gam.pred <- predict(gam.mod, Xtest)
dtest <- xgb.DMatrix(data = as.matrix(Xtest[, feats[-4], with = FALSE]))
xgb.pred <- predict(xgb.mod, dtest)
```

The residuals of all models are shown below 
together with cubic regression splines.
In general most residuals are quite small and values near $D_{FDR} = 0$ and
$D_{FDR} = 1$ are fitted well.
However, there are many values -- especially around $0.2 < D_{FDR} < 0.7$ --
with large residuals.
From the standard errors it seems that these values are the minority.
Nevertheless, there is clearly a structure visible not caputred by any model.

```{r, warning=FALSE}
dt <- data.table(
  D_FDR = rep(Ytest, 3),
  residual = c(Ytest - mars.pred[, 1], Ytest - gam.pred, Ytest - xgb.pred),
  model = rep(c("MARS", "GAM", "XGBoost"), each = length(Ytest))
)
ggplot(dt, aes(x = D_FDR, y = residual)) +
  geom_point(aes(color = model), alpha = .3, size = 1) +
  geom_smooth(aes(color = model, fill = model)) +
  theme_bw()
```

Standard errors are calculated and shown below.
Basically, it would be more useful to know the standard error in the range of
$0 \le D_{FDR} \le 0.1$ since the scDD method would not be used if it's
predicted FDR is more than 10% off.
We included the standard error only considering this range.

```{r}
err <- rbind(
  dt[, .(se = sd(residual) / sqrt(length(residual))), by = model],
  dt[D_FDR < 0.1, .(se = sd(residual) / sqrt(length(residual))), by = model]
)
err$D_FDR <- rep(c("full", "below 10%"), each = 3)
ggplot(err, aes(x = model, y = se)) +
  geom_pointrange(aes(ymin = 0, ymax = se), color = "blue") +
  coord_flip() +
  facet_grid(D_FDR ~ .) +
  theme_bw()
(err)
```

The MARS model is the worst, the tree ensemble is slightly better than 
the generalized additive model.
For predicting the loss of FDR control both XGBoost and GAM are near 0.003
standard error.
Confidence intervals for below 10% FDR control loss are shown below.

```{r}
err[D_FDR == "below 10%", .(CI.99 = 2.58 * se, CI.95 = 1.96 * se), by = model]
```

XGBoost is the best but it is also the substantially most complex model.
The GAM is much smaller and easier to interpret.
I will continue with both.












***













# Save

Now the final models are trained on the whole dataset and saved.

**GAM**

```{r}
gam.mod <- gam(
  D_FDR ~ 
    ti(n_hat, cv_hat_between) +
    ti(cv_hat_between, batch_corrs) +
    te(n_hat, cv_hat_within, batch_corrs),
  data = FDR,
  method = "ML"
)
saveRDS(gam.mod, "scDD_gam.mod.rds")
```

**XGBoost**

```{r}
dtrain <- xgb.DMatrix(
  data = as.matrix(FDR[, feats[-4], with = FALSE]), 
  label = FDR$D_FDR
)
xgb.mod <- xgb.train(params = parsl, data = dtrain, nrounds = best$iter)
saveRDS(xgb.mod, "scDD_xgb.mod.rds")
```





















***












# Session Info

```{r}
sessionInfo()
```


