---
title: "Batch Effect Exploration"
subtitle: "Modeling FDR Control of scDD"
author: "Marc Schwering"
output:
  html_document:
    number_sections: true
    toc: true
    toc_depth: 2
    fig_width: 10
    fig_height: 7
    theme: cosmo
---




# Introduction

In this script I try to predict the loss of FDR control for scDD using
pooled expression
with Kolmogorov-Smirnov test.

## Load

First, libraries are loaded.

```{r, message=FALSE, results='hide'}
library(ggplot2) # plotting
library(data.table) # database-like tables
library(earth) # MARS
library(mda) # fda
```

Objects were prepared in the pre-processing step and are loaded.

```{r}
AUC <- readRDS("AUC.rds")
FDR <- readRDS("FDR.rds")
noCall <- readRDS("noCall.rds")
```

## Data

```{r}
X <- FDR[method == "scDD_pool_ks" & type == "full", !"D_FDR"]
Y <- FDR[method == "scDD_pool_ks" & type == "full", D_FDR]
```

Predictor variables are stored in a vector.

```{r}
feats <- c("n_hat", "cv_hat_within", "cv_hat_between",
           "kBET_rate", "batch_corrs", "predFDR", "median_count", "mean_count")
```














***















# Regression via MARS

A Multivariate  Adaptive Regression Splines (MARS) model is fit to the data.
As there are not too many variables and the dataset is not so big,
an exhaustive term selection can be used during the pruning method and repeated
cross validation is used to estimate R squared.
The method is forced to stop during the forward pass if the estimated R squared
does not increase by 0.005.
This leads to including a little less than 30 terms during the forward pass.
Interaction terms are allowed if the respective first degree terms are already
in the model.

## Fitting

We only focus on the observations below 20\% FDR loss.
This is where the model should be reliable.
This could be done by weighting, but the MARS is painfully slow using weights.
A more rigorous way is to just exclude the datapoints above
20\% FDR loss.

```{r}
incl <- ifelse(Y > 0.2, FALSE, TRUE)
table(incl)
```

```{r, results='hide'}
fit <- earth(X[incl, feats, with = FALSE], Y[incl],
             degree = 2, nk = 100, thresh = 0.005,
             pmethod = "exhaustive", nfold = 5, ncross = 30)
```

```{r}
summary(fit)
```

There is a ridiculously high interaction term between $batch_{corrs}$
and $\hat{cv}_{within}$.
They are correlated.
For the next runs I will supress interaction terms of these two variables.

```{r}
allowed <- function(degree, pred, parents, namesx) {
  if (degree < 2) return(TRUE)
  idx1 <- which(namesx == "cv_hat_within")
  idx2 <- which(namesx == "batch_corrs")
  if ((pred == idx1 && parents[idx2]) || (pred == idx2 && parents[idx1])) {
    return(FALSE)
  }
  TRUE
}
```

```{r, results='hide'}
fit2 <- earth(X[incl, feats, with = FALSE], Y[incl],
             degree = 2, nk = 100, thresh = 0.005, allowed = allowed,
             pmethod = "exhaustive", nfold = 5, ncross = 30)
```

```{r}
summary(fit2)
```

Median count does not seem to be so useful, so I will remove it.
In the following I also tried to remove $batch_{corrs}$ and $kBET_{rate}$
without losing to much performance (CVRsq).
Creating these 2 features is computationally expensive.
This could be really inconvenient on large datasets.
So it would save a lot of time if I can omit them.
Below is the model without $kBET_{rate}$ and median count.

```{r, results='hide'}
fit3 <- earth(X[incl, feats[c(-4, -7)], with = FALSE], Y[incl],
             degree = 2, nk = 100, thresh = 0.005, allowed = allowed,
             pmethod = "exhaustive", nfold = 5, ncross = 30)
```

```{r}
summary(fit3)
plot(fit3)
```

The residuals show some structure.
This is something I cannot get rid of.
I also tried out other model families (gam and xgboost).
They all lead to the same residuals plot.
Either the data is just too noisy or I am missing a feature.

I can fit the other part of the data quite well (the $D_{FDR} > 0.2$ part),
but that is not really useful.
Eventually, one would want to know whether or not scDD can be used,
so $|D_{FDR}|$ near 0 is important.

## Estimated Prediction Error

At this point I want to know how good I can infer with these models.
For that I can fit a model to the residuals,
and estimate the confidence intervals for future predictions.
Fortunately, this is already implemented in `earth` as a variance model.

From the residual plot above it seems that the variance of the
absolute residuals increases and falls with the response going from 0 to 0.2.
So, I will fit a MARS to the residuals (as well).
The plot below shows the 95\% confidence intervals in gray, and the respective
prediction intervals in red.

```{r, results='hide'}
fit3 <- earth(X[incl, feats[c(-4, -7)], with = FALSE], Y[incl],
             degree = 2, nk = 100, thresh = 0.005, allowed = allowed,
             pmethod = "exhaustive", nfold = 5, ncross = 30,
             varmod.method = "earth")
```

```{r}
plot(fit3, 3, level = .95)
```

Now it would be handy to remove $batch_{corrs}$ because all the
features left are cheap to compute.
So, below I want to see  how bad the prediction intervals get when
I remove $batch_{corrs}$.

```{r, results='hide'}
fit4 <- earth(X[incl, feats[c(-4, -5, -7)], with = FALSE], Y[incl],
             degree = 2, nk = 100, thresh = 0.005,
             pmethod = "exhaustive", nfold = 5, ncross = 30,
             varmod.method = "earth")
```

```{r}
plot(fit4, 3, level = .95)
```

The error is similar but still,
the prediction error ranges from 5\% to 10\% (95\% confidence).
In the best case such a prediction could be used in a "yes" or "no"
manner.






***









# Classification via FDA

It might be easier to convert this to a classification problem.
Eventually, I want to know whether or not I can use scDD.
The question is where to set the boundary.

As we see departures from true FDR of edgeR (our baseline here) as well,
we could say, if scDD's loss of FDR control is reasonably higher than edgeR's
we should not use it.
Or one could just set it to some threshold like 10\% loss of FDR control.
I just chose 10\% as from edgeR's results (which is kind of a baseline here)
we also see a slight loss of FDR control somwhere around 5\% but mostly
below 10\%.

```{r}
X <- X[, feats[c(-4, -5, -7)], with = FALSE]
YedgeR <- FDR[method == "edgeR_sum" & type == "full", D_FDR]
X$YedgeR <- abs(Y) <= abs(YedgeR)
table(X$YedgeR)
X$Y01 <- abs(Y) <= 0.1
table(X$Y01)
```

Since, I was already using the MARS model, I will continue
fitting a flexible discriminant analysis (FDA).
Here, the basis expansion from a MARS is basically used with a discriminant
analysis.

Below I use the cases where scDD's FDR control is not worse than edgeR's
as labels.
To get around 30 terms during the forward pass,
the threshold is set to 3\%.

```{r, results='hide'}
fit <- fda(
  YedgeR ~ n_hat + cv_hat_within + cv_hat_between + predFDR + mean_count,
  data = X, keep.fitted = TRUE, keepxy = FALSE, method = earth,
  degree = 2, nk = 100, thresh = 0.003, pmethod = "exhaustive",
  nfold = 5, ncross = 30
)
```

```{r}
summary(fit$fit)
post <- as.data.frame(predict(fit, X, type = "posterior"))
hist(post$`FALSE`, breaks = 30)
```

From the posterior probability distribution we can see that in many cases
the model is not sure how to classify.
It might be worth finding a good cutoff for the classifier.
Below is the distribution of both label instances is
plotted along the discriminant coordinate.

```{r}
vars <- predict(fit, X, type = "variates")
df <- data.frame(
  variate = vars[, 1],
  label = X$YedgeR
)
ggplot(df) +
  geom_histogram(aes(x = variate, fill = label), bins = 30) +
  facet_grid(label ~ ., scales = "free_y") +
  theme_bw()
```

Note, the y-axis scales.
The large overlap shows that it will generally be hard to distinguish
both instances.
Also note, that this is just the training data.
To actually estimate the error I will do a repeated cross validation.








***








# Prediction Error Estimation

A repeated cross validation will be used to estimate the prediction error
of the models.
For this, a few functions are defined first.
Below is a function that extracts the labels, posterior probabilities,
and ranking from predictions of a test set.

```{r}
ExtractResults <- function(model, Xtest, Ytest) {
  dt <- data.table(
    PostProb = predict(model, Xtest, type = "posterior")[, 2],
    Label = Ytest
  )
  dt <- na.omit(dt)
  dt$Rank <- rank(dt$PostProb)
  dt
}
```

The function below is used to compute confusion matrices for a spectrum
of cutoff points (thresholds for labelling).

```{r}
confuse <- function(ranks, labels, PostProbs, n = 1000) {
  # prepare table
  stopifnot(length(ranks) == length(labels))
  dt <- data.table(rank = ranks, label = labels, probs = PostProbs)
  steps <- seq.int(2, nrow(dt), length.out = n)
  dt <- dt[order(rank)]

  # fill matrix with TPs, ...
  m <- matrix(NA, n, 4, dimnames = list(dt$probs[steps], c("TP", "FP", "TN", "FN")))
  for (s in seq_along(steps)) {
    res <- dt[, .(TP = sum(rank >= steps[s] & label),
                    FP = sum(rank >= steps[s] & !label),
                    TN = sum(rank < steps[s] & !label),
                    FN = sum(rank < steps[s] & label))]
    m[s, ] <- as.numeric(res[1, ])
  }
  m
}
```

The function below calculates predictive values from
confusion matrices.

```{r}
GetPredVals <- function(CM, cross) {
  dt <- data.table(
    Step = seq_len(nrow(CM)),
    PostProb = rownames(CM),
    Recall = CM[, 1] / (CM[, 1] + CM[, 4]),
    Fallout = CM[, 2] / (CM[, 2] + CM[, 3]),
    Precision = CM[, 1] / (CM[, 1] + CM[, 2]),
    FOR = CM[, 4] / (CM[, 4] + CM[, 3]),
    Accuracy = (CM[, 1] + CM[, 3]) / rowSums(CM),
    Cross = cross
  )
  dt
}
```

Finally, everything is used in a nested loop which will do the repeated
cross validation for a model.

```{r}
CrossVal <- function(data, labels, nfold = 5, ncross = 10) {
  crossResults <- vector("list", ncross)
  for (cross in seq_len(ncross)) {
    print(paste("Cross", cross, "..."))
    folds <- sample(seq_len(nfold), nrow(data), replace = TRUE)

    # folds
    foldPreds <- vector("list", nfold)
    for (fold in seq_len(nfold)) {
      Xtrain <- data[folds != fold, ]
      Ytrain <- labels[folds != fold]
      Xtest <- data[fold == fold, ]
      Ytest <- labels[fold == fold]
      Xtrain <- cbind(Ytrain, Xtrain)
      colnames(Xtrain)[1] <- "label"
      sink("/dev/null") # make it shut up
      model <- fda(label ~ ., data = Xtrain, keep.fitted = TRUE, keepxy = FALSE,
                   method = earth, degree = 2, nk = 100, thresh = 0.003,
                   pmethod = "exhaustive", nfold = 5, ncross = 30)
      sink()
      foldPreds[[fold]] <- ExtractResults(model, Xtest, Ytest)
    }
    foldPreds <- rbindlist(foldPreds)
    CMs <- confuse(foldPreds$Rank, foldPreds$Label, foldPreds$PostProb)
    PredVals <- GetPredVals(CMs, cross)
    crossResults[[cross]] <- PredVals
  }

  rbindlist(crossResults)
}
```

With that, both models are cross validated:
one using edgeR's FDR control as labels, one using 10\% FDR control as labels.

```{r}
resYedgeR <- CrossVal(X[, c(-6, -7)], X[, YedgeR])
resY01 <- CrossVal(X[, c(-6, -7)], X[, Y01])
```

ROC curves are shown below.

```{r}
resY01[, PostProb := as.numeric(PostProb)]
resYedgeR[, PostProb := as.numeric(PostProb)]
res <- rbind(resYedgeR, resY01)
res$Target <- c(rep("YedgeR", nrow(resYedgeR)), rep("Y01", nrow(resY01)))
res$group <- paste0(res$Target, res$Cross)
ggplot(res, aes(group = group, color = Target)) +
  geom_abline(slope = 1, color = "gray", linetype = 2) +
  geom_path(aes(x = Fallout, y = Recall)) +
  theme_bw()
```

Classifying whether scDD's loss of FDR control is below 10\% seems easier.
As this is a reasonable model I will chose this one.
I guess I will want to make some plots later on, so I will save the
cross validation results here.

```{r}
saveRDS(res, "scDD_class_CV.rds")
```











***









# Final Model

Finally, the model is fitted on the whole dataset.

```{r, results='hide'}
model <- fda(Y01 ~ ., data = X[, !"YedgeR"], keep.fitted = FALSE, keepxy = FALSE,
                   method = earth, degree = 2, nk = 100, thresh = 0.003,
                   pmethod = "exhaustive", nfold = 5, ncross = 30)
saveRDS(model, "scDD_class.rds")
```

The posterior probability distribution from the cross validation results
is shown below.

```{r}
ggplot(resY01) +
  geom_histogram(aes(x = PostProb), bins = 30) +
  labs(x = "Posterior Probability") +
  theme_bw()
```

Specificity (1 - fallout) is plotted as a function of the posterior
probability.
A specificity of 90\% would be a desired cutoff for a classifier, so
a horizontal line at 0.9 is plotted as well.
The posterior probability which would label the data according to
reach 90\% specificity is shown as red line.
Note that the posterior probability is logged.

```{r}
(thresh <- resY01[Fallout < 0.11 & Fallout > 0.099, median(PostProb)])
ggplot(resY01, aes(x = PostProb, y = 1 - Fallout)) +
  geom_point(alpha = .3, size = .7) +
  geom_hline(yintercept = 0.9, color = "gray", linetype = 2) +
  geom_vline(xintercept = thresh, color = "red", linetype = 2) +
  scale_x_continuous(trans = "log10") +
  labs(x = "Posterior Probability", y = "Specificity") +
  theme_bw()
```

Below is the accuracy (balanced)
plotted as a function of the posterior probability.
The posterior probability which would label the data according to
reach 90\% specificity is shown as red line.
Again the x axis is logged.

```{r}
resY01[, max(Recall - Fallout)]
ggplot(resY01, aes(x = PostProb, y = Recall - Fallout)) +
  geom_point(alpha = .3, size = .7) +
  geom_vline(xintercept = thresh, color = "red", linetype = 2) +
  scale_x_continuous(trans = "log10") +
  labs(x = "Posterior Probability", y = "Accuracy") +
  theme_bw()
```

It is in the region of a maximum balanced accuracy of around 79\%.
The ROC curve with 90\% specificity is shown below.

```{r}
ggplot(resY01, aes(group = Cross)) +
  geom_abline(slope = 1, color = "gray", linetype = 2) +
  geom_hline(yintercept = 0.9, color = "red", linetype = 3) +
  geom_path(aes(x = Fallout, y = Recall), alpha = .5) +
  theme_bw()
```


























***












# Session Info

```{r}
sessionInfo()
```
